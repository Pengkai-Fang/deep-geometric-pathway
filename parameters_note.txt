parameter modification note

GAT

1. 
	4 layer(2 global 2 inducitve) lr=0.01 cv=5
	layer1(1 --> 4(head=4))
	layer2(16 --> 64(head=4))
	layer3(256 --> 18(head=18))
	layer4(18*18 --> 1)
	results: loss 0.0082 score 0.38
	


2. 
	4 layer(2 global 2 inducitve) lr=0.01 cv=5
	layer1(1 --> 16(head=1))
	layer2(16 --> 64(head=1))
	layer3(64 --> 18(head=18))
	layer4(18*18 --> 1)
	results: loss 0.0072 score 0.41


TODO


SAGE

1.
	4 layer(2 global 2 inducitve) lr=0.01 cv=5
	layer1(1 --> 16)
	layer2(16 --> 32)
	layer3(32 --> 64)
	layer4(64 --> 1)
	results: loss 0.0072 score 0.41

2. 
	self.conv1 = SAGEConv(in_channels, 128, normalize=True, concat=concat, node_dim=1)
        self.conv2 = SAGEConv(128, 256, normalize=True, concat=concat, node_dim=1)
        self.conv3 = SAGEConv(256, 512, normalize=False, concat=False, node_dim=1)
        self.conv4 = SAGEConv(512, out_channels, normalize=False, concat=False, node_dim=1)
	dropout 0.5 each layer
	results: loss 0.0068 score 0.45

3.
	self.conv1 = SAGEConv(in_channels, 6, normalize=True, concat=concat, node_dim=1)
        self.conv2 = SAGEConv(6, 36, normalize=True, concat=concat, node_dim=1)
        self.conv3 = SAGEConv(36, 216, normalize=False, concat=False, node_dim=1)
        self.conv4 = SAGEConv(216, out_channels, normalize=False, concat=False, node_dim=1)
	no dropout
	results: loss 0.0066 score 0.57



4. 
	self.conv1 = SAGEConv(in_channels, 6, normalize=True, concat=concat, node_dim=1)
        self.conv2 = SAGEConv(6, 36, normalize=True, concat=concat, node_dim=1)
        self.conv3 = SAGEConv(36, 216, normalize=False, concat=False, node_dim=1)
        self.conv4 = SAGEConv(216, out_channels, normalize=False, concat=False, node_dim=1)
	no dropout smoothL1loss 
	results: loss 0.0032 score 0.5
	

5. 
	self.conv1 = SAGEConv(in_channels, 6, normalize=True, concat=concat, node_dim=1)
        self.conv2 = SAGEConv(6, 36, normalize=True, concat=concat, node_dim=1)
        self.conv3 = SAGEConv(36, 64, normalize=True, concat=concat, node_dim=1)

        self.lin1 = nn.Sequential(
            nn.Linear(64*77, 1024),
            nn.ReLU(True),
            nn.Linear(1024, 218),
            nn.ReLU(True),
            nn.Linear(218, 1)
        )
	no dropout smoothL1Loss
	results: loss 0.0026, score 0.66


1. ~~Add GraphEdge test~~ (DONE)
2. Only sample the protein
3. Try smoothL1loss **(WORK)**/ ~L1loss~(NOT WORK)
4. ~Without inductive learning~(DONE) better

11037
  |  10450 - 11309
		| 10450 - 11307
    