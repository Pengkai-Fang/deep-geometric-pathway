{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch_geometric\n",
    "import scipy.io as sio\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import math\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import Parameter\n",
    "from torch_scatter import scatter_add\n",
    "from torch_geometric.nn.conv import MessagePassing\n",
    "from torch_geometric.utils import add_remaining_self_loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset(nn):\n",
    "    def _reset(item):\n",
    "        if hasattr(item, 'reset_parameters'):\n",
    "            item.reset_parameters()\n",
    "\n",
    "    if nn is not None:\n",
    "        if hasattr(nn, 'children') and len(list(nn.children())) > 0:\n",
    "            for item in nn.children():\n",
    "                _reset(item)\n",
    "        else:\n",
    "            _reset(nn)\n",
    "            \n",
    "def glorot(tensor):\n",
    "    if tensor is not None:\n",
    "        stdv = math.sqrt(6.0 / (tensor.size(-2) + tensor.size(-1)))\n",
    "        tensor.data.uniform_(-stdv, stdv)\n",
    "\n",
    "def zeros(tensor):\n",
    "    if tensor is not None:\n",
    "        tensor.data.fill_(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataroot = './rna-genome-data/'\n",
    "data_x = pd.read_csv(dataroot + 'availPPidata.csv')\n",
    "data_edge_index = pd.read_csv(dataroot + 'availedge.csv')\n",
    "\n",
    "sourceroot = './NewCleanDataSetBRCA/'\n",
    "rna_data = sio.loadmat(sourceroot + 'Data_RNASeq2.mat')\n",
    "cancerfreedata = rna_data['geneRNASeq2RawMatrix0']\n",
    "cancerobtdata = rna_data['geneRNASeq2RawMatrix1']\n",
    "target = np.concatenate([np.zeros((cancerfreedata.shape[1])),np.ones((cancerobtdata.shape[1]))]).astype(int)\n",
    "data_x = data_x.values\n",
    "data = np.concatenate([data_x, target.reshape(1,-1)],axis=0)\n",
    "data = data.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCNConv(MessagePassing):\n",
    "    def __init__(self, in_channels, out_channels, improved=False, cached=False,\n",
    "                 bias=True, normalize=True, **kwargs):\n",
    "        super(GCNConv, self).__init__(aggr='add', **kwargs)\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.improved = improved\n",
    "        self.cached = cached\n",
    "        self.normalize = normalize\n",
    "        self.node_dim = 1\n",
    "\n",
    "        self.weight = Parameter(torch.Tensor(in_channels, out_channels))\n",
    "\n",
    "        if bias:\n",
    "            self.bias = Parameter(torch.Tensor(out_channels))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "\n",
    "        self.reset_parameters()\n",
    "        \n",
    "    def reset_parameters(self):\n",
    "        glorot(self.weight)\n",
    "        zeros(self.bias)\n",
    "        self.cached_result = None\n",
    "        self.cached_num_edges = None\n",
    "\n",
    "    @staticmethod\n",
    "    def norm(edge_index, num_nodes, edge_weight=None, improved=False,\n",
    "             dtype=None):\n",
    "        if edge_weight is None:\n",
    "            edge_weight = torch.ones((edge_index.size(1), ), dtype=dtype,\n",
    "                                     device=edge_index.device)\n",
    "\n",
    "        fill_value = 1 if not improved else 2\n",
    "        edge_index, edge_weight = add_remaining_self_loops(\n",
    "            edge_index, edge_weight, fill_value, num_nodes)\n",
    "\n",
    "        row, col = edge_index\n",
    "        deg = scatter_add(edge_weight, row, dim=0, dim_size=num_nodes)\n",
    "        deg_inv_sqrt = deg.pow(-0.5)\n",
    "        deg_inv_sqrt[deg_inv_sqrt == float('inf')] = 0\n",
    "\n",
    "        return edge_index, deg_inv_sqrt[row] * edge_weight * deg_inv_sqrt[col]\n",
    "\n",
    "    def forward(self, x, edge_index, edge_weight=None):\n",
    "        \"\"\"\"\"\"\n",
    "        x = torch.matmul(x, self.weight)\n",
    "\n",
    "        if self.cached and self.cached_result is not None:\n",
    "            if edge_index.size(1) != self.cached_num_edges:\n",
    "                raise RuntimeError(\n",
    "                    'Cached {} number of edges, but found {}. Please '\n",
    "                    'disable the caching behavior of this layer by removing '\n",
    "                    'the `cached=True` argument in its constructor.'.format(\n",
    "                        self.cached_num_edges, edge_index.size(1)))\n",
    "\n",
    "        if not self.cached or self.cached_result is None:\n",
    "            self.cached_num_edges = edge_index.size(1)\n",
    "            if self.normalize:\n",
    "                edge_index, norm = self.norm(edge_index, x.size(self.node_dim),\n",
    "                                             edge_weight, self.improved,\n",
    "                                             x.dtype)\n",
    "            else:\n",
    "                norm = edge_weight\n",
    "                \n",
    "            self.cached_result = edge_index, norm\n",
    "\n",
    "        edge_index, norm = self.cached_result\n",
    "\n",
    "        return self.propagate(edge_index, x=x, norm=norm)\n",
    "\n",
    "\n",
    "    def message(self, x_j, norm):\n",
    "        print(x_j.shape)\n",
    "        return norm.view(-1, 1) * x_j if norm is not None else x_j\n",
    "\n",
    "    def update(self, aggr_out):\n",
    "        if self.bias is not None:\n",
    "            aggr_out = aggr_out + self.bias\n",
    "        return aggr_out\n",
    "\n",
    "    def __repr__(self):\n",
    "        return '{}({}, {})'.format(self.__class__.__name__, self.in_channels,\n",
    "                                   self.out_channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class net(nn.Module):\n",
    "    def __init__(self, in_channel, out_channel, node_num):\n",
    "        super(net, self).__init__()\n",
    "        self.node_num = node_num\n",
    "        self.layer1 = GCNConv(in_channel, 32, bias=True, normalize=True)\n",
    "        self.layer2 = GCNConv(32, 64, bias=True, normalize=True)\n",
    "        self.layer3 = GCNConv(64, out_channel, bias=True,normalize=True)\n",
    "        self.lin1 = nn.Linear(out_channel * self.node_num, 1)\n",
    "        \n",
    "    def forward(self, x, edge_index):\n",
    "        x = x.float()\n",
    "        x = F.relu(self.layer1(x, edge_index),True)\n",
    "        x = F.relu(self.layer2(x, edge_index),True)\n",
    "        x = F.relu(self.layer3(x, edge_index),True)\n",
    "        x = x.view(-1, out_channel*self.node_num)\n",
    "        x = self.lin1(x)\n",
    "        x = F.sigmoid(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_val_split(opdata, val_ratio=0.05, test_ratio=0.1, shuffle=False):\n",
    "    \"\"\"\n",
    "    make the seperation of dataset, the ratio is 0.1 of dataset to be the \n",
    "    \"\"\"\n",
    "    num_obs = opdata.shape[0]\n",
    "    n_v = int(math.floor(val_ratio * num_obs))\n",
    "    n_t = int(math.floor(test_ratio * num_obs))\n",
    "    if shuffle:\n",
    "        opdata = np.random.shuffle(opdata)\n",
    "    valdata = opdata[:n_v,:]\n",
    "    testdata = opdata[n_v:n_v+n_t,:]\n",
    "    traindata = opdata[n_v+n_t:,:]\n",
    "    return traindata, valdata, testdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "traindata, valdata, testdata = train_val_split(data)\n",
    "node_num = data.shape[1]\n",
    "model = net(1, 64, node_num).to(device)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "traindata_t = torch.from_numpy(traindata).unsqueeze(-1)\n",
    "valdata_t = torch.from_numpy(valdata).unsqueeze(-1)\n",
    "testdata_t = torch.from_numpy(testdata).unsqueeze(-1)\n",
    "traintarget = Variable(traindata_t[:,-1,:]).to(device)\n",
    "valtarget = Variable(valdata_t[:,-1,:]).to(device)\n",
    "testtarget = Variable(testdata_t[:,-1,:]).to(device)\n",
    "traindata_t = Variable(traindata_t[:,:-1,:]).to(device)\n",
    "valdata_t = Variable(valdata_t[:,:-1,:]).to(device)\n",
    "testdata_t = Variable(testdata_t[:,:-1,:]).to(device)\n",
    "\n",
    "edge_index = torch.from_numpy(data_edge_index.values.T).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    model.train()\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    out = model(traindata_t, edge_index)\n",
    "    loss = criterion(out, traintarget) \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    model.eval()\n",
    "    loss = criterion(model(valdata_t, edge_index), valtarget)\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(200):\n",
    "    loss = train()\n",
    "    log = 'Epoch: {:03d}, Train: {:.4f}, Test: {:.4f}'\n",
    "    print(log.format(epoch+1, loss, test()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
